
# ğŸŒ Page Ranking in Search Engines

### 1. ğŸ§  PageRank & Graph Traversals
Ever wondered how search engines decide which page appears at the top when you search for something? A huge part of that magic is **PageRank**, a graph-based algorithm developed by Googleâ€™s founders. But wait, graph traversal methods like **DFS** and **BFS** also sneak into the backend when dealing with crawling or link analysis.

Letâ€™s decode how this works ğŸ‘‡

## ğŸ“Œ i. PageRank
The web is treated like a **directed graph** where each page is a node and hyperlinks are edges. PageRank assigns a score to each page based on how many other pages link to it â€” and how important those pages are.

Imagine it like a popularity contest: If a popular person vouches for you (links to your page), your reputation (PageRank) improves.

Here's a simple version of PageRank in code:

```python
def compute_pagerank(graph, d=0.85, iterations=100):
    N = len(graph)
    pagerank = {node: 1 / N for node in graph}

    for _ in range(iterations):
        new_rank = {}
        for node in graph:
            rank_sum = 0
            for other in graph:
                if node in graph[other]:
                    rank_sum += pagerank[other] / len(graph[other])
            new_rank[node] = (1 - d) / N + d * rank_sum
        pagerank = new_rank

    return pagerank
```

### ğŸŒ The Algo, My Way

- Treat web pages as **nodes** and hyperlinks as **edges** in a graph.
- PageRank simulates a **random surfer** who clicks on links at random.
- The rank is based on the probability that the surfer lands on a page.
![PageRank Image](images/pagerank.png)


### â±ï¸ Time & Space Complexity - PageRank
- **Time Complexity**: O(N Ã— E Ã— I), where N = number of nodes, E = edges, I = iterations
- **Space Complexity**: O(N)

---

## ğŸ“Œ ii. DFS & BFS for Web Crawling
While PageRank ranks the pages, algorithms like **DFS (Depth First Search)** and **BFS (Breadth First Search)** help in **web crawling** â€” i.e., discovering pages and links.

- **DFS**: Goes deep into a link chain before backtracking. Might miss out on breadth.
- **BFS**: Explores all neighbors first. Better for finding shortest paths or shallow-level content.

```python
# BFS for crawling
from collections import deque

def bfs_crawl(graph, start):
    visited = set()
    queue = deque([start])

    while queue:
        node = queue.popleft()
        if node not in visited:
            print("Visited:", node)
            visited.add(node)
            queue.extend(neigh for neigh in graph[node] if neigh not in visited)
```

### ğŸŒ The Algo, My Way

- Use **BFS** when exploring all links from a homepage or seed page.
- Use **DFS** when you want to go deep into specific branches of sites.
![Graph Traversal](images/dfs_bfs.png)


### â±ï¸ Time & Space Complexity - DFS/BFS
- **Time Complexity**: O(N + E)
- **Space Complexity**: O(N)

---

## ğŸ”„ Comparison: PageRank vs BFS/DFS

| Feature              | PageRank                 | DFS/BFS               |
|---------------------|---------------------------|------------------------|
| **Purpose**         | Ranking web pages         | Crawling or discovery |
| **Input**           | Directed graph            | Graph                  |
| **Output**          | Rank of each node         | Traversal order       |
| **Time Complexity** | O(N Ã— E Ã— I)              | O(N + E)              |
| **Space**           | O(N)                      | O(N)                  |
| **Iterative?**      | Yes (repeated updates)    | Usually once          |

### âœ… When to Use Which?
- **Use PageRank** when you need to **score and prioritize** pages.
- **Use BFS/DFS** when youâ€™re focused on **discovering or crawling** pages efficiently.

